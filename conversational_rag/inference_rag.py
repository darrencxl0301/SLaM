#inference_rag.py
import argparse
import random
import time
import numpy as np
import torch
import warnings
import json
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from sentence_transformers import SentenceTransformer
import faiss
from dataclasses import dataclass
from typing import List, Dict

warnings.filterwarnings('ignore')

@dataclass
class ConversationPair:
    question: str
    answer: str
    metadata: Dict = None

class ConversationRAG:#all-MiniLM-L6-v2
    def __init__(self, embedding_model="shibing624/text2vec-base-chinese"):
        self.embedding_model = SentenceTransformer(embedding_model)
        self.conversations = []
        self.index = None
        self.embeddings = None
    
    def load_conversations_from_jsonl(self, jsonl_path: str) -> List[ConversationPair]:
        conversations = []
        try:
            with open(jsonl_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        data = json.loads(line.strip())
                        if 'conversations' in data:
                            conversation_list = data['conversations']
                            user_msg = None
                            assistant_msg = None
                            
                            for msg in conversation_list:
                                if msg.get('role') == 'user':
                                    user_msg = msg.get('content', '').strip()
                                elif msg.get('role') == 'assistant':
                                    assistant_msg = msg.get('content', '').strip()
                                    break
                            
                            if user_msg and assistant_msg:
                                conversations.append(ConversationPair(
                                    question=user_msg,
                                    answer=assistant_msg,
                                    metadata={'line_number': line_num}
                                ))
                    except:
                        continue
        except Exception as e:
            print(f"Error reading file: {e}")
            return []
        
        print(f"Loaded {len(conversations)} conversation pairs")
        return conversations
    
    def build_index(self, jsonl_path: str, index_path: str = "cache/faiss_index.bin", conversations_cache_path: str = "cache/conversations_cache.json"):
        if os.path.exists(index_path) and os.path.exists(conversations_cache_path):
            print("Loading FAISS index and conversations from cache...")
            self.index = faiss.read_index(index_path)
            with open(conversations_cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                self.conversations = [ConversationPair(**item) for item in cached_data]
            print(f"Index loaded with {self.index.ntotal} vectors. {len(self.conversations)} conversations loaded from cache.")
            return True
        
        print("Loading conversations...")
        self.conversations = self.load_conversations_from_jsonl(jsonl_path)
        
        if not self.conversations:
            print("No conversations loaded. Cannot build index.")
            return False
        
        print("Generating embeddings...")
        questions = [conv.question for conv in self.conversations]
        self.embeddings = self.embedding_model.encode(questions, convert_to_tensor=False)
        self.embeddings = np.array(self.embeddings).astype('float32')
        
        print("Building FAISS index...")
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        faiss.normalize_L2(self.embeddings)
        self.index.add(self.embeddings)
        
        print(f"Index built with {self.index.ntotal} vectors")
        
        # Save the index and conversations
        faiss.write_index(self.index, index_path)
        with open(conversations_cache_path, 'w', encoding='utf-8') as f:
            json.dump([conv.__dict__ for conv in self.conversations], f, ensure_ascii=False, indent=4)
        print("FAISS index and conversations saved to cache.")
        
        return True
    
    def retrieve(self, query: str, top_k: int = 3, threshold: float = 0.3):
        if self.index is None or not self.conversations:
            return []
        
        # ç¡®ä¿queryæ˜¯å­—ç¬¦ä¸²ç±»å‹å¹¶æ¸…ç†
        if not isinstance(query, str):
            query = str(query)
        
        query = query.strip()
        
        # å¦‚æœqueryä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ
        if not query:
            return []
        
        try:
            # ç¡®ä¿ä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
            query_embedding = self.embedding_model.encode(
                [query], 
                convert_to_tensor=False,
                show_progress_bar=False
            )
            query_embedding = np.array(query_embedding).astype('float32')
            faiss.normalize_L2(query_embedding)
            
            scores, indices = self.index.search(query_embedding, top_k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if score >= threshold and idx < len(self.conversations):
                    results.append((self.conversations[idx], float(score)))
            
            return results
        
        except Exception as e:
            print(f"Error during retrieval: {e}")
            print(f"Query type: {type(query)}, Query value: '{query}'")
            return []
    
    def get_context_for_query(self, query: str, top_k: int = 3, max_context_length: int = 1000, threshold: float = 0.3):
        try:
            retrieved = self.retrieve(query, top_k=top_k, threshold=threshold)
            
            if not retrieved:
                return ""
            
            context_parts = ["ä»¥ä¸‹æ˜¯çŸ¥è¯†åº“ä¸­çš„ç›¸å…³ç¤ºä¾‹ï¼š\n"]
            
            for i, (conv_pair, score) in enumerate(retrieved, 1):
                example = f"ç¤ºä¾‹ {i}:\né—®ï¼š{conv_pair.question}\nç­”ï¼š{conv_pair.answer}\n"
                current_context = "\n".join(context_parts) + example
                if len(current_context) > max_context_length:
                    break
                context_parts.append(example)
            
            return "\n".join(context_parts)
        
        except Exception as e:
            print(f"Error getting context: {e}")
            return ""

def init_qwen_model(args):
    """Initialize Qwen model with LoRA adapters."""
    base_model_name = "Qwen/Qwen3-4B"
    
    print(f"Loading tokenizer from {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load base model with 4-bit quantization
    print("Loading base model with 4-bit quantization...")
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=quantization_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        # attn_implementation="flash_attention_2"
    )
    # Load LoRA adapters if provided
    if args.lora_path:
        print(f"Loading LoRA adapters from {args.lora_path}...")
        model = PeftModel.from_pretrained(base_model, args.lora_path)
    else:
        print("No LoRA path provided, using base model only")
        model = base_model
    
    model.eval()
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f'Total Parameters: {total_params / 1e6:.2f}M')
    print(f'Trainable Parameters: {trainable_params / 1e6:.2f}M')
    
    return model, tokenizer


def create_qwen_rag_prompt(query: str, rag_system: ConversationRAG, use_rag: bool = True, rag_threshold: float = 0.3):
    """Create Qwen-formatted prompt with RAG context - éµå¾ªæ¨¡å‹è®­ç»ƒæ ¼å¼"""
    
    system_prompt = "You are Qwen, created by Alibaba Cloud. You are a helpful assistant."
    
    instruction = """è¯·ä»¥è·¯æ˜“å¨ç™»ï¼ˆLouis Vuittonï¼‰ä¸“ä¸šå®¢æˆ·æœåŠ¡ä»£è¡¨çš„èº«ä»½ï¼Œä¸“æ³¨äºOnTheGoæ‰‹è¢‹ç³»åˆ—æ¥å›ç­”é—®é¢˜ã€‚

**æœåŠ¡æ ‡å‡†ï¼š**
- ä¿æŒä¼˜é›…ã€ä¸“ä¸šå’Œç»†è‡´çš„å¥¢ä¾ˆå“ç‰ŒæœåŠ¡æ€åº¦
- å±•ç°å¯¹OnTheGoç³»åˆ—çš„æ·±å…¥äº†è§£ï¼ˆå°ºå¯¸ã€æè´¨ã€ä¿å…»ç­‰ï¼‰
- ä½¿ç”¨ä¸“ä¸šä¸”æ¸©æš–çš„è¯­æ°”

**å›ç­”è¦æ±‚ï¼š**
1. æ ¹æ®çŸ¥è¯†åº“ä¸­çš„äº§å“ä¿¡æ¯ä½œç­”
2. æä¾›å…¨é¢çš„äº§å“ç»†èŠ‚ï¼ˆå°ºå¯¸ã€æè´¨ã€ç‰¹ç‚¹ã€ä¿å…»ï¼‰
3. ä¸»åŠ¨æ»¡è¶³å®¢æˆ·éœ€æ±‚ï¼Œé¢„åˆ¤ç›¸å…³é—®é¢˜
4. ä½¿ç”¨æ­£ç¡®çš„äº§å“åç§°å’Œæœ¯è¯­
5. ä»…æä¾›æœ‰ä¾æ®çš„å‡†ç¡®ä¿¡æ¯
6. ä»¥è§£å†³æ–¹æ¡ˆä¸ºå¯¼å‘

**OnTheGoç³»åˆ—è¦ç‚¹ï¼š**
- å°ºå¯¸ï¼šPMã€MMã€GM
- ç‰¹ç‚¹ï¼šå¯ç¿»è½¬è®¾è®¡ã€å®½æ•ç©ºé—´ã€åŒé‡æºå¸¦æ–¹å¼
- æè´¨ï¼šMonogramå¸†å¸ƒã€å‹çº¹çš®é©ã€Damieræ£‹ç›˜æ ¼ç­‰

ã€OnTheGo å°ºå¯¸å‚è€ƒã€‘
- PMï¼š25 Ã— 19 Ã— 11.5 cmï¼ˆè½»ç›ˆå°å·§ï¼‰
- MMï¼š35 Ã— 27 Ã— 14 cmï¼ˆå®¹é‡é€‚ä¸­ï¼‰
- GMï¼š41 Ã— 34 Ã— 19 cmï¼ˆå¤§å®¹é‡æ—…è¡ŒåŒ…ï¼‰
"""


    
    # æ·»åŠ RAGä¸Šä¸‹æ–‡
    if use_rag and rag_system:
        context = rag_system.get_context_for_query(query, top_k=3, max_context_length=800, threshold=rag_threshold)
        if context.strip():
            instruction += f"\n**çŸ¥è¯†åº“å‚è€ƒä¿¡æ¯ï¼š**\n{context}\n"
    
    # ç»„åˆæœ€ç»ˆçš„ç”¨æˆ·æ¶ˆæ¯
    user_message = f"{instruction}\n**å®¢æˆ·é—®é¢˜ï¼š**\n{query}\n\nè¯·æä¾›ä¸“ä¸šã€è¯¦ç»†çš„å›ç­”ï¼š"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_message}
    ]
    
    return messages

def clean_response_simple(response: str) -> str:
    """
    ç®€åŒ–ç‰ˆæœ¬ï¼šåªä¿ç•™åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆå¥å·/æ„Ÿå¹å·/é—®å·ï¼Œç§»é™¤é«˜åº¦ç›¸ä¼¼çš„é‡å¤å¥å­ï¼ˆ80%ä»¥ä¸Šï¼‰ï¼Œ
    ç§»é™¤**word**ã€[word]ã€ã€wordã€‘æ ¼å¼çš„å†…å®¹åŠå…¶åé¢çš„æ‰€æœ‰æ–‡æœ¬ï¼Œ
    ç§»é™¤\nåä¸ä»¥æœ‰æ•ˆæ ‡ç‚¹ç»“å°¾çš„æ®µè½ï¼Œ
    ç§»é™¤"å†è§"åŠå…¶åæ‰€æœ‰å†…å®¹ï¼Œ
    ç§»é™¤ç¬¬äºŒä¸ª"æ‚¨å¥½"åŠå…¶åæ‰€æœ‰å†…å®¹ï¼Œ
    ç§»é™¤åŒ…å«ç”µè¯å·ç (+å¼€å¤´)çš„å¥å­
    
    Args:
        response: åŸå§‹ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        
    Returns:
        æ¸…ç†åçš„å›å¤æ–‡æœ¬
    """
    if not response or response == "[NO RESPONSE]":
        return response
    
    import re
    
    # ç§»é™¤"å†è§"åŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬å†è§æœ¬èº«ï¼‰
    goodbye_pos = response.find('å†è§')
    if goodbye_pos != -1:
        response = response[:goodbye_pos].strip()
    
    # ç§»é™¤ç¬¬äºŒä¸ª"æ‚¨å¥½"åŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
    first_hello = response.find('æ‚¨å¥½')
    if first_hello != -1:
        # ä»ç¬¬ä¸€ä¸ª"æ‚¨å¥½"ä¹‹åå¼€å§‹æŸ¥æ‰¾ç¬¬äºŒä¸ª
        second_hello = response.find('æ‚¨å¥½', first_hello + 2)
        if second_hello != -1:
            response = response[:second_hello].strip()
    
    # ç§»é™¤åŒ…å«ç”µè¯å·ç çš„å¥å­ï¼ˆ+å¼€å¤´çš„ç”µè¯å·ç æ ¼å¼ï¼‰
    # åŒ¹é…æ¨¡å¼: +åé¢è·Ÿæ•°å­—ã€çŸ­æ¨ªçº¿ã€ç©ºæ ¼ç­‰
    phone_pattern = r'[^ã€‚ï¼ï¼Ÿ]*\+\d+[0-9\-\s()ï¼ˆï¼‰]*[^ã€‚ï¼ï¼Ÿ]*[ã€‚ï¼ï¼Ÿ]?'
    response = re.sub(phone_pattern, '', response)
    
    # ç§»é™¤ **word** æ ¼å¼çš„å†…å®¹åŠå…¶åé¢çš„æ‰€æœ‰æ–‡æœ¬
    asterisk_pattern = r'\*\*.*?\*\*.*'
    response = re.sub(asterisk_pattern, '', response, flags=re.DOTALL)
    
    # ç§»é™¤ [word] æ ¼å¼çš„å†…å®¹åŠå…¶åé¢çš„æ‰€æœ‰æ–‡æœ¬
    bracket_pattern = r'\[.*?\].*'
    response = re.sub(bracket_pattern, '', response, flags=re.DOTALL)
    
    # ç§»é™¤ ã€wordã€‘ æ ¼å¼çš„å†…å®¹åŠå…¶åé¢çš„æ‰€æœ‰æ–‡æœ¬
    chinese_bracket_pattern = r'ã€.*?ã€‘.*'
    response = re.sub(chinese_bracket_pattern, '', response, flags=re.DOTALL)
    
    # å¤„ç†æ¢è¡Œç¬¦ï¼Œç§»é™¤æ¢è¡Œåä¸ä»¥æœ‰æ•ˆæ ‡ç‚¹ç»“å°¾çš„æ®µè½
    if '\n' in response:
        paragraphs = response.split('\n')
        valid_paragraphs = []
        
        for para in paragraphs:
            para = para.strip()
            if para:  # éç©ºæ®µè½
                # æ£€æŸ¥æ®µè½æ˜¯å¦ä»¥æœ‰æ•ˆæ ‡ç‚¹ç»“å°¾
                if para.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                    valid_paragraphs.append(para)
                else:
                    # æ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆæ ‡ç‚¹çš„ä½ç½®
                    last_valid = -1
                    for ending in ('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'):
                        pos = para.rfind(ending)
                        if pos > last_valid:
                            last_valid = pos
                    
                    if last_valid != -1:
                        # æˆªå–åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆæ ‡ç‚¹
                        valid_paragraphs.append(para[:last_valid + 1])
        
        response = '\n'.join(valid_paragraphs)
    
    # æ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆçš„å¥å­ç»“æŸç¬¦
    valid_endings = ('ã€‚', 'ï¼', 'ï¼Ÿ')
    last_valid_pos = -1
    
    for ending in valid_endings:
        pos = response.rfind(ending)
        if pos > last_valid_pos:
            last_valid_pos = pos
    
    # å¦‚æœæ‰¾åˆ°æœ‰æ•ˆç»“æŸç¬¦ï¼Œæˆªå–åˆ°è¯¥ä½ç½®ï¼ˆåŒ…å«ç»“æŸç¬¦ï¼‰
    if last_valid_pos != -1:
        response = response[:last_valid_pos + 1].strip()
    
    # ç§»é™¤ç›¸ä¼¼çš„é‡å¤å¥å­ï¼ˆ80%ä»¥ä¸Šç›¸ä¼¼åº¦ï¼‰
    # æŒ‰å¥å­åˆ†å‰²ï¼ˆä¿ç•™æ ‡ç‚¹ç¬¦å·ï¼‰
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', response)
    
    # é‡ç»„å¥å­
    complete_sentences = []
    for i in range(0, len(sentences) - 1, 2):
        if i + 1 < len(sentences):
            sentence = (sentences[i] + sentences[i + 1]).strip()
            if sentence:
                complete_sentences.append(sentence)
    
    # è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦
    def similarity(s1: str, s2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å­—ç¬¦çº§åˆ«çš„æ¯”è¾ƒï¼‰"""
        if not s1 or not s2:
            return 0.0
        
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·è¿›è¡Œæ¯”è¾ƒ
        s1_clean = s1.rstrip('ã€‚ï¼ï¼Ÿ').strip()
        s2_clean = s2.rstrip('ã€‚ï¼ï¼Ÿ').strip()
        
        if s1_clean == s2_clean:
            return 1.0
        
        # ä½¿ç”¨ç®€å•çš„å­—ç¬¦åŒ¹é…è®¡ç®—ç›¸ä¼¼åº¦
        len1, len2 = len(s1_clean), len(s2_clean)
        max_len = max(len1, len2)
        
        if max_len == 0:
            return 0.0
        
        # è®¡ç®—æœ€é•¿å…¬å…±å­åºåˆ—çš„é•¿åº¦
        matches = 0
        for char in set(s1_clean):
            matches += min(s1_clean.count(char), s2_clean.count(char))
        
        return matches / max_len
    
    # å»é™¤ç›¸ä¼¼åº¦è¶…è¿‡80%çš„è¿ç»­å¥å­
    cleaned_sentences = []
    prev_sentence = None
    
    for sentence in complete_sentences:
        if prev_sentence is None:
            # ç¬¬ä¸€ä¸ªå¥å­æ€»æ˜¯ä¿ç•™
            cleaned_sentences.append(sentence)
            prev_sentence = sentence
        else:
            # è®¡ç®—ä¸å‰ä¸€å¥çš„ç›¸ä¼¼åº¦
            sim = similarity(prev_sentence, sentence)
            
            if sim < 0.8:  # ç›¸ä¼¼åº¦å°äº80%æ‰ä¿ç•™
                cleaned_sentences.append(sentence)
                prev_sentence = sentence
            # å¦åˆ™ä¸¢å¼ƒï¼ˆç›¸ä¼¼åº¦>=80%ï¼‰
    
    result = ''.join(cleaned_sentences)
    
    return result if result else response



def generate_response(model, tokenizer, query, rag_system, args, use_rag=True):
    """Generate response using Qwen model with RAG."""
    try:
        # ç¡®ä¿queryæ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        if not isinstance(query, str) or not query.strip():
            return "[Invalid query]"
        
        # Create messages with RAG context
        messages = create_qwen_rag_prompt(query, rag_system, use_rag, args.rag_threshold)
        
        # ä½¿ç”¨ apply_chat_template - æ¨èæ–¹å¼
        if hasattr(tokenizer, 'apply_chat_template') and tokenizer.chat_template is not None:
            # æ–¹å¼1: ç›´æ¥tokenizeå¹¶è¿”å›tensorï¼ˆæ¨èï¼‰
            inputs = tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
                enable_thinking=args.enable_thinking 
            ).to(model.device)
            
            # å¦‚æœä½ æƒ³çœ‹ç”Ÿæˆçš„promptæ ¼å¼ï¼ˆè°ƒè¯•ç”¨ï¼‰
            if False:  # è®¾ç½®ä¸ºTrueå¯ä»¥æŸ¥çœ‹prompt
                prompt_text = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    enable_thinking=args.enable_thinking 
                )
                print(f"\nç”Ÿæˆçš„Prompt:\n{prompt_text}\n")
        else:
            # Fallback: å¦‚æœæ²¡æœ‰chat_templateï¼ˆç†è®ºä¸ŠQwenéƒ½æœ‰ï¼‰
            print("âš ï¸ Warning: No chat_template found, using manual formatting")
            prompt = f"<|im_start|>system\n{messages[0]['content']}<|im_end|>\n"
            prompt += f"<|im_start|>user\n{messages[1]['content']}<|im_end|>\n"
            prompt += "<|im_start|>assistant\n"
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature if args.temperature > 0 else None,
                top_p=args.top_p if args.temperature > 0 else None,
                do_sample=args.temperature > 0,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
                # use_cache=False,
                repetition_penalty=1.2  # é˜²æ­¢é‡å¤
            )
        
        # Decode only the generated part
        input_length = inputs['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
        
        # æ¸…ç†å›å¤
        response = clean_response_simple(response)
        
        return response if response else "[NO RESPONSE]"
        
    except Exception as e:
        import traceback
        print(f"Error generating response: {e}")
        print(f"Traceback: {traceback.format_exc()}")
        return f"[Error: {e}]"

def interactive_chat(model, tokenizer, rag_system, args):
    """Interactive chat mode with RAG."""
    print("\n" + "="*60)
    print("ğŸ¤– Qwen Model with RAG - Interactive Chat")
    print("="*60)
    print("\nCommands:")
    print("  'quit' or 'q' - Exit")
    print("  'toggle_rag' - Toggle RAG on/off")
    print("  'search <query>' - Test RAG retrieval")
    print("  'clear' - Clear screen")
    print("-" * 60 + "\n")
    
    use_rag = True
    
    while True:
        user_input = input("ğŸ‘¤ You: ").strip()
        
        if user_input.lower() in ['quit', 'exit', 'q']:
            print("\nGoodbye! ğŸ‘‹")
            break
        
        elif user_input.lower() == 'toggle_rag':
            use_rag = not use_rag
            status = "âœ… ON" if use_rag else "âŒ OFF"
            print(f"ğŸ”§ RAG is now {status}")
            continue
        
        elif user_input.lower() == 'clear':
            os.system('clear' if os.name != 'nt' else 'cls')
            continue
        
        elif user_input.lower().startswith('search '):
            query = user_input[7:].strip()
            if rag_system:
                results = rag_system.retrieve(query, top_k=5, threshold=args.rag_threshold)
                print(f"\nğŸ” Search results for: '{query}'")
                print(f"Threshold: {args.rag_threshold}")
                if results:
                    for i, (conv_pair, score) in enumerate(results, 1):
                        print(f"\nğŸ“Œ Result {i} (Score: {score:.3f}):")
                        print(f"   Q: {conv_pair.question[:100]}...")
                        print(f"   A: {conv_pair.answer[:150]}...")
                else:
                    print("âŒ No relevant results found (try lowering threshold)")
            else:
                print("âŒ RAG system not available")
            print("-" * 60)
            continue
        
        if not user_input:
            continue
        
        # Show RAG status
        rag_icon = "ğŸ” RAG" if use_rag else "ğŸš« No RAG"
        print(f"\n{rag_icon} | ğŸ¤– Assistant: ", end="", flush=True)
        
        # Generate response
        start_time = time.time()
        response = generate_response(model, tokenizer, user_input, rag_system, args, use_rag=use_rag)
        elapsed_time = time.time() - start_time
        
        print(response)
        print(f"\nâ±ï¸  Response time: {elapsed_time:.2f}s")
        
        # Show retrieved context if RAG was used
        if use_rag and rag_system:
            retrieved = rag_system.retrieve(user_input, top_k=2, threshold=args.rag_threshold)
            if retrieved:
                print(f"\nğŸ“š Retrieved {len(retrieved)} relevant examples:")
                for i, (conv_pair, score) in enumerate(retrieved, 1):
                    print(f"   {i}. (Score: {score:.3f}) {conv_pair.question[:60]}...")
        
        print("-" * 60 + "\n")

def batch_evaluation(model, tokenizer, rag_system, args):
    """Batch evaluation mode."""
    if args.manual_input:
        print("\n" + "="*60)
        print("ğŸ“ Manual Input with RAG Comparison")
        print("="*60)
        print("Enter your questions. Type 'quit' to exit.")
        print("-" * 60 + "\n")
        
        while True:
            user_input = input("ğŸ‘¤ Enter your question: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                break
            
            if not user_input:
                continue
            
            print(f"\nâ“ Question: {user_input}")
            print("=" * 60)
            
            if args.compare_rag:
                print("\nğŸš« Without RAG:")
                response_no_rag = generate_response(model, tokenizer, user_input, rag_system, args, use_rag=False)
                print(f"{response_no_rag}")
                
                print("\n" + "-" * 60)
                print("ğŸ” With RAG:")
                response_with_rag = generate_response(model, tokenizer, user_input, rag_system, args, use_rag=True)
                print(f"{response_with_rag}")
                
                if rag_system:
                    retrieved = rag_system.retrieve(user_input, top_k=3, threshold=args.rag_threshold)
                    if retrieved:
                        print("\nğŸ“š Retrieved Context:")
                        for j, (conv_pair, score) in enumerate(retrieved, 1):
                            print(f"   {j}. (Score: {score:.3f}) {conv_pair.question[:80]}...")
                    else:
                        print("\nğŸ“š No relevant context found")
            else:
                response = generate_response(model, tokenizer, user_input, rag_system, args, use_rag=True)
                print(f"ğŸ¤– Assistant: {response}")
            
            print("=" * 60 + "\n")
    else:
        # Predefined test prompts
        test_prompts = [
            "What are the key HR policies in Malaysia?",
            "How should companies handle employee termination?",
            "What are the EPF contribution requirements?",
            "Explain the process for handling workplace disputes",
            "What are the mandatory employee benefits?",
        ]
        
        print("\n" + "="*60)
        print(f"ğŸ“Š Batch Evaluation with RAG ({len(test_prompts)} prompts)")
        print("="*60 + "\n")
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"[{i}/{len(test_prompts)}] â“ {prompt}")
            
            if args.compare_rag:
                print("\nğŸš« Without RAG:")
                response_no_rag = generate_response(model, tokenizer, prompt, rag_system, args, use_rag=False)
                print(f"{response_no_rag}")
                
                print("\nğŸ” With RAG:")
                response_with_rag = generate_response(model, tokenizer, prompt, rag_system, args, use_rag=True)
                print(f"{response_with_rag}")
                
                if rag_system:
                    retrieved = rag_system.retrieve(prompt, top_k=2, threshold=args.rag_threshold)
                    if retrieved:
                        print("\nğŸ“š Retrieved Context:")
                        for j, (conv_pair, score) in enumerate(retrieved, 1):
                            print(f"   {j}. (Score: {score:.3f}) {conv_pair.question[:80]}...")
            else:
                response = generate_response(model, tokenizer, prompt, rag_system, args, use_rag=True)
                print(f"ğŸ¤– Assistant: {response}")
            
            print("=" * 60 + "\n")

def main():
    parser = argparse.ArgumentParser(description="Qwen Model with RAG Evaluation")
    
    # Model configuration
    parser.add_argument('--lora_path', type=str, default="out/lora/latest_qwen_4b_zh.pth", 
                        help='Path to LoRA adapters (leave empty to use base model only)')
    parser.add_argument('--temperature', type=float, default=0.7, help='Sampling temperature')
    parser.add_argument('--top_p', type=float, default=0.8, help='Top-p sampling parameter')
    parser.add_argument('--max_new_tokens', type=int, default=150, help='Maximum new tokens to generate')
    parser.add_argument('--max_seq_len', type=int, default=1024, help='Maximum input sequence length')
    
    # RAG configuration
    parser.add_argument('--rag_dataset', type=str, required=True, 
                        help='Path to JSONL file for RAG knowledge base')
    parser.add_argument('--disable_rag', action='store_true', help='Disable RAG functionality')
    parser.add_argument('--rag_threshold', type=float, default=0.3, 
                        help='Minimum similarity score for RAG retrieval (0.0-1.0)')
    
    # Evaluation mode
    parser.add_argument('--mode', type=str, choices=['interactive', 'batch'], default='interactive',
                        help='Evaluation mode')
    parser.add_argument('--compare_rag', action='store_true', 
                        help='Compare responses with and without RAG (batch mode)')
    parser.add_argument('--manual_input', action='store_true', 
                        help='Allow manual input in batch mode')
    parser.add_argument('--enable_thinking', action='store_true', default=True)
    
    # Other parameters
    parser.add_argument('--seed', type=int, default=42, help='Random seed')
    
    args = parser.parse_args()
    
    # Set random seed
    if args.seed is not None:
        random.seed(args.seed)
        np.random.seed(args.seed)
        torch.manual_seed(args.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(args.seed)
    
    print("ğŸš€ Initializing Qwen model...")
    model, tokenizer = init_qwen_model(args)
    
    # Initialize RAG system
    rag_system = None
    if not args.disable_rag:
        try:
            print("\nğŸ“š Setting up RAG system...")
            if not os.path.exists(args.rag_dataset):
                print(f"âŒ RAG dataset not found: {args.rag_dataset}")
                print("Continuing without RAG...")
            else:
                rag_system = ConversationRAG()
                if rag_system.build_index(args.rag_dataset):
                    print("âœ… RAG system initialized successfully!")
                else:
                    rag_system = None
                    print("âŒ Failed to build RAG index, continuing without RAG...")
        except Exception as e:
            print(f"âŒ Error initializing RAG system: {e}")
            print("Continuing without RAG...")
            rag_system = None
    else:
        print("RAG system disabled")
    
    # Run evaluation
    if args.mode == 'interactive':
        interactive_chat(model, tokenizer, rag_system, args)
    elif args.mode == 'batch':
        batch_evaluation(model, tokenizer, rag_system, args)

if __name__ == "__main__":
    main()